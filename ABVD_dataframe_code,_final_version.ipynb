{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABVD dataframe code, final version",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX0aFfNxrl13"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import reduce #for summing a list of pandas dataframes\n",
        "import re #regular expressions\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "import difflib\n",
        "\n",
        "!pip install lingtypology #only because colab doesn't have it by default\n",
        "import lingtypology"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HWNCc7krqCL"
      },
      "source": [
        "def importwordlists(lang_ids):\n",
        "    \"\"\"downloads the ABVD's word lists of languages, indicated by their ABVD id,\n",
        "    and returns them in a database.\"\"\"\n",
        "    colnames = [\"id\",\"word_id\",\"word\",\"item\",\"annotation\",\"loan\",\"cognacy\",\"pmpcognacy\"]\n",
        "    mydata = pd.DataFrame(columns = colnames)\n",
        "    csv_url = \"https://abvd.shh.mpg.de/utils/save/?type=csv&section=austronesian&language=\"\n",
        "    for name,ABVD_id in lang_ids.items():\n",
        "        print(\"Downloading ABVD wordlist id =\",ABVD_id,\"of\",name, end='')\n",
        "        data = pd.read_csv(csv_url+str(ABVD_id), names=colnames, index_col=False, escapechar='\\\\')\n",
        "        data = data.iloc[int(np.where((data==colnames).all(axis=1))[0])+1:]\n",
        "        data[\"language name\"]=name\n",
        "        data[\"ABVD id\"]=ABVD_id\n",
        "        mydata = pd.concat([mydata, data])\n",
        "        print(\" - done.\")\n",
        "    return mydata\n",
        "\n",
        "def charfreq(languagename, sortbyfreq=False):\n",
        "    \"\"\"returns two lists with all unique characters in a given language's wordlist\n",
        "    and their respective frequencies. By default the list is sorted alphabetically.\n",
        "    Sorting by descending frequency is also possible.\"\"\"\n",
        "    df_lang = mydata[mydata[\"language name\"]==languagename]\n",
        "    allitems = ''.join(df_lang[\"item\"])\n",
        "    if sortbyfreq:\n",
        "        chars = sorted(set(allitems), key=allitems.count, reverse=True)\n",
        "    else:\n",
        "        chars = sorted(set(allitems))\n",
        "    charfreqs = [allitems.count(char) for char in chars]\n",
        "    return chars,charfreqs\n",
        "\n",
        "def findsubstrings(strings, languagename):\n",
        "    \"\"\"given a list of strings (e.g. ['N','ñ']) and a language name (e.g. 'Paser'),\n",
        "    returns a pandas database with the items from the given language that contain any of these strings.\"\"\"\n",
        "    df_lang = mydata[mydata[\"language name\"]==languagename]\n",
        "    df_strings = [df_lang[\"item\"].str.contains(i) for i in strings] #reminder: str.contains can also handle regular expressions!\n",
        "    return df_lang[reduce(lambda x, y: x.add(y), df_strings)]\n",
        "\n",
        "def findPMPwords(regex):\n",
        "    \"\"\"given a regular expression (e.g. '^[qk]'), this returns a pandas dataframe\n",
        "    containing the PMP items that match the regular expression.\"\"\"\n",
        "    df_PMP = mydata[mydata[\"language name\"]==\"Proto-Malayo-Polynesian\"]\n",
        "    df_matchitems = df_PMP[\"item\"].str.contains(regex)\n",
        "    df_PMP = df_PMP[df_matchitems]\n",
        "    df_PMP = df_PMP[df_PMP.cognacy.notna()]\n",
        "    return df_PMP\n",
        "\n",
        "def matchreflexes(df_PMP,language,cogn_only=True):\n",
        "    \"\"\"returns descendants of the words in df_PMP in the specified language.\n",
        "    Optionally returns a list of words with the same id, not just descendants\"\"\"\n",
        "    df_lang = mydata[mydata[\"language name\"]==language].set_index(\"word_id\")\n",
        "    df_lang = df_lang.loc[df_lang.index.intersection(df_PMP.word_id)]\n",
        "    df_lang = df_lang[df_lang.cognacy.notna()] #exclude rows with NaN cognacy value\n",
        "    df_lang.reset_index(inplace=True)\n",
        "    if cogn_only:\n",
        "        for wordid in sorted(set(df_lang.word_id)):\n",
        "            PMPcognrs = [nr for row in df_PMP.set_index(\"word_id\").cognacy.loc[[wordid]] for nr in row.split(',')]\n",
        "            for ind,row in df_lang[df_lang.word_id==wordid].iterrows():\n",
        "                if not any(nr in PMPcognrs for nr in row.cognacy.split(',')):\n",
        "                    df_lang.drop(ind, inplace=True)\n",
        "    return df_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TanggeuJjTMy"
      },
      "source": [
        "#Get the data\n",
        "lang_ids = {\"Kadorih\":487, \"Dayak Ngaju\":360, \"Katingan\":158, \"Yakan\":200, \"Ma'anyan\":215, \"Paser (~Taboyan)\":1209, \"Tunjung\":189, \"Malagasy (Tandroy)\":1186, \"Proto Malagasy\":1526, \"Proto-Malayo-Polynesian\":269}\n",
        "#note 1: A few languages have multiple entries in the ABVD (under different ids). I manually selected the most complete entries.\n",
        "#note 2: I didn't use the Tandroy Malagasy and Proto Malagasy data in the end.\n",
        "mydata = importwordlists(lang_ids)\n",
        "display(mydata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM_r750HgmBC"
      },
      "source": [
        "#Put Smith's segments with respective contexts in lists, for both vowels and consonants.\n",
        "#I didn't have time to analyze the vowels for my Bachelor's thesis, but the program is ready for it\n",
        "#Adding additional segments+contexts can also be done here, e.g. *j-, *-t or *m.\n",
        "\n",
        "#Smith's PMP phonemes and contexts (word-initial, between vowels and word-final)\n",
        "PMP_C_AlexSmith = [\"-p-\",\"-t-\",\"-k-\",\"q-\",\"-q-\",\"-q\",\"b-\",\"-b-\",\"-b\",\"d-\",\"-d-\",\"-d\",\"-j-\",\"-j\",\"z-\",\"-z-\",\"s-\",\"-s-\",\"-s\",\"l-\",\"-l-\",\"-l\",\"R-\",\"-R-\",\"-R\",\"y\",\"w\"] #note: ABVD writes ʀ, not R\n",
        "PMP_V_AlexSmith = [\"-a\",\"-aC\",\"-aCVC\",\"-u\",\"-uC\",\"-uCVC\",\"-i\",\"-iC\",\"-iCVC\",\"-əC\",\"-əCVC\",\"-ay\",\"aw\"] #note: the ABVD list writes e for schwas, as is tradition\n",
        "\n",
        "#the corresponding regular expressions, to search the ABVD list for the right words\n",
        "PMP_C_AlexSmith_re = [\"[aeiouí]p[aeiouí]\",\"[aeiouí]t[aeiouí]\",\"[aeiouí]k[aeiouí]\",\"^\\*q\",\"[aeiouí]q[aeiouí]\",\"q$\",\"^\\*b\",\"[aeiouí]b[aeiouí]\",\"b$\",\"^\\*d\",\"[aeiouí]d[aeiouí]\",\"d$\",\"[aeiouí]j[aeiouí]\",\"j$\",\"^\\*z\",\"[aeiouí]z[aeiouí]\",\"^\\*s\",\"[aeiouí]s[aeiouí]\",\"s$\",\"^\\*l\",\"[aeiouí]l[aeiouí]\",\"l$\",\"^\\*ʀ\",\"[aeiouí]ʀ[aeiouí]\",\"ʀ$\",\"y\",\"w\"]\n",
        "PMP_V_AlexSmith_re = [\"a$\",\"a[^aeiouí]$\",\"a[^aeiouí][aeiouí][^aeiouí]$\",\"u$\",\"u[^aeiouí]$\",\"u[^aeiouí][aeiouí][^aeiouí]$\",\"i$\",\"i[^aeiouí]$\",\"i[^aeiouí][aeiouí][^aeiouí]$\",\"e[^aeiouí]$\",\"e[^aeiouí][aeiouí][^aeiouí]$\",\"ay$\",\"aw\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMoP0mYtNfVo"
      },
      "source": [
        "#Generate a table of the number of reflexes per language per segment, as found in the cell above.\n",
        "\n",
        "nr_of_reflexes = np.zeros(shape=(len(list(lang_ids.keys())[:-3]), len(PMP_C_AlexSmith)), dtype=int)\n",
        "for i,phoneme in enumerate(PMP_C_AlexSmith):\n",
        "    df_PMP = findPMPwords(PMP_C_AlexSmith_re[i])\n",
        "    for j,language in enumerate(list(lang_ids.keys())[:-3]):\n",
        "        df_lang = matchreflexes(df_PMP,language)\n",
        "        df_PMP_matches = df_PMP.set_index(\"word_id\").loc[df_lang.word_id].reset_index()\n",
        "        nr_of_reflexes[j,i] = len(df_lang)\n",
        "nr_of_reflexes = pd.DataFrame(data = nr_of_reflexes.T, index=PMP_C_AlexSmith, columns=list(lang_ids.keys())[:-3])\n",
        "display(nr_of_reflexes)\n",
        "nr_of_reflexes.to_csv('number of reflexes consonants.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaIsql1zmS6n"
      },
      "source": [
        "#Finds the PMP items containing a given sequence of PMP characters,\n",
        "#and returns the words in the modern languages with the same word_id.\n",
        "#These are candidates for cognate sets, but the cognacy and loan checks must be\n",
        "#done manually! I only used this to generate cognacy sets, not in my analysis.\n",
        "\n",
        "PMPstrings = [\"^\\*z\"] #here goes the list of PMP sequences for which to return candidate cognate sets\n",
        "df_with_phonemes = findsubstrings(PMPstrings,\"Proto-Malayo-Polynesian\")\n",
        "\n",
        "display(df_with_phonemes[[\"word_id\",\"word\",\"item\",\"annotation\",\"cognacy\"]])\n",
        "ids = list(df_with_phonemes[\"word_id\"])\n",
        "\n",
        "modernlangs = mydata[mydata[\"language name\"].isin(list(lang_ids.keys())[:-3]+[\"Proto-Malayo-Polynesian\"])]\n",
        "for wordid in ids:\n",
        "    df_wrds = modernlangs[modernlangs['word_id']==wordid]\n",
        "    print(df_wrds[\"word\"].iloc[0])\n",
        "    df = df_wrds[[\"item\",\"language name\",\"cognacy\",\"loan\",\"annotation\"]].T\n",
        "    display(HTML(df.to_html(header=False)))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBUSrCOK4rwL"
      },
      "source": [
        "#from https://oneadder.github.io/lingtypology/html/index.html\n",
        "\n",
        "m = lingtypology.LingMap(('Ot Danum', 'Ngaju', 'Yakan', \"Ma'anyan\", 'Tawoyan', 'Tunjung'))\n",
        "m.start_location = (0,115)\n",
        "m.start_zoom = 5\n",
        "m.add_features(['Kadorih', 'Ngaju', 'Yakan', \"Ma'anyan\", 'Taboyan', 'Tunjung'])\n",
        "m.create_map()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS_a81G5CskC"
      },
      "source": [
        "#I tried reordering Smith's step-ladder, but didn't find anything interesting\n",
        "#and didn't use this in the end.\n",
        "\n",
        "nwb=\"+--------\"\n",
        "swb=\"++---+---\"\n",
        "yak=\"+++------\"\n",
        "seb=\"-+++---++\"\n",
        "ceb=\"-++++--++\"\n",
        "neb=\"--+++++--\"\n",
        "tun=\"+---++++-\"\n",
        "names = [\"nwb\",\"swb\",\"yak\",\"seb\",\"ceb\",\"neb\",\"tun\"]\n",
        "smith = [nwb,swb,yak,seb,ceb,neb,tun]\n",
        "d = {n:s for n,s in zip(names,smith)}\n",
        "\n",
        "for i,l in enumerate(d.items()):\n",
        "    print(l[0])\n",
        "    d_s = dict(sorted(d.items(), key=lambda x: difflib.SequenceMatcher(None,x[1],l[1]).ratio(),reverse=True)[1:])\n",
        "    print(list(d_s.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}